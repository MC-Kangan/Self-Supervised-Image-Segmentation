{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"RJAFjBViRSv4"},"outputs":[],"source":["# Import the necessary libraries\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","# Import torch vision\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","# Import resnet50 model from torchvision\n","from torchvision.models import resnet50\n","from torch.optim.optimizer import Optimizer, required\n","import re\n","# Import the dataset from huggingface\n","# pip install datasets\n","from datasets import load_dataset\n","import os \n","# import the SimCLRDataset\n","from SimCLRDataset import *\n","# dataset = load_dataset(\"cats_vs_dogs\")\n","# print(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fE9r5gjERSv6"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"S6fydlJbRSv6"},"source":["# Data augmentation"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"pkz03YueRSv7"},"outputs":[],"source":["# configure the image height and width for resizing the images to input the model\n","# As the pytorch resnet model requires the input image to be 224x224 ,even with pre-trained weights equal False\n","# we will resize the images to 224x224, as size bigger than 224x224 will be cropped to 224x224\n","image_height = 224\n","image_width = 224\n","\n","def get_color_distortion(s=1.0):\n","    # s is the strength of color distortion.\n","    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n","    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n","    rnd_gray = transforms.RandomGrayscale(p=0.2)\n","    color_distort = transforms.Compose([\n","    rnd_color_jitter,\n","    rnd_gray])\n","    return color_distort\n","# This is the best combination of data augmentation techniques for the SimCLR model shown in the paper\n","data_transforms = transforms.Compose([\n","    transforms.ToPILImage(),  # Convert tensor to PIL Image\n","    transforms.RandomResizedCrop((image_height, image_width)), # This follow the random cropping and resizing in the paper\n","    get_color_distortion(s=1),\n","    transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)), # In the paper, the kernel size is 10% of the image height and width and sigma is between 0.1 and 2.0. As the kernel size must be odd, we choose 23 as the kernel size.\n","    transforms.ToTensor()\n","])\n","\n","# dataloader for the dataset and print the first batch of images\n","# The batch size is 64 as mentioned in the paper\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# dataset = preprocess(load_dataset(\"cats_vs_dogs\")['train'], device = device)\n","# print(dataset)\n"]},{"cell_type":"markdown","metadata":{"id":"QmYh2QQvRSv7"},"source":["# Define the loss function for SimCLR"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"6xKw7I6bRSv8"},"outputs":[],"source":["def nt_xent_loss(queries, keys, temperature = 0.1):\n","    b, device = queries.shape[0], queries.device\n","\n","    n = b * 2  # 同一图片内部不同patch也是负样本\n","    projs = torch.cat((queries, keys))\n","    logits = projs @ projs.t()\n","\n","    mask = torch.eye(n, device=device).bool()\n","    logits = logits[~mask].reshape(n, n - 1)  # 同一图片内部不同patch也是负样本，除了自己和自己\n","    logits /= temperature\n","\n","    labels = torch.cat(((torch.arange(b, device = device) + b - 1), torch.arange(b, device=device)), dim=0)\n","    loss = F.cross_entropy(logits, labels, reduction = 'sum')\n","    loss /= n\n","    return loss\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gT170nLJRSv8"},"source":["# Get the backbone model f(.) to train on the data augmentation dataset."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":904,"status":"ok","timestamp":1711724711993,"user":{"displayName":"Jingfang Gu","userId":"08830157379463741241"},"user_tz":0},"id":"_Ty2CrcTRSv8","outputId":"23d31771-cd8c-494a-850d-71d351e3b619"},"outputs":[],"source":["\n","# Load the resnet50 model which returns the features before the classification layer\n","model = resnet50(pretrained=False) # Optionally, you can set pretrained=True to use the pre-trained weights\n","# return the features before the classification layer\n","model.fc = nn.Identity() # Remove the classification layer\n","# Print the model architecture\n","# print(model)\n","# get the output shape of the model by passing a random tensor of the image size\n","# print(model(torch.randn(1, 3, image_height, image_width)).shape)\n","\n","\n","\n","\n","# Define the model\n","class SimCLR(nn.Module):\n","    def __init__(self, model, temperature=0.1):\n","        super(SimCLR, self).__init__()\n","        # get the device of the model\n","        self.model = model\n","        # This is the two-layer MLP projection head as described in the paper whcih represents the g(.) function\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(2048, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 128)\n","        )\n","        self.temperature = temperature\n","        # Define the cosine similarity function\n","        # cosine_similarity = lambda z_i, z_j: torch.dot(z_i, z_j) / (torch.norm(z_i) * torch.norm(z_j))\n","    def forward(self, x_i, x_j):\n","        h_i = self.model(x_i)\n","        h_j = self.model(x_j)\n","        # print(h.shape)\n","        z_i = self.projection_head(h_i)\n","        # print(z_i.shape)\n","        z_j = self.projection_head(h_j)\n","        # get the normalized projection head output\n","        # z_i = nn.functional.normalize(z_i, dim=1)\n","        # z_j = nn.functional.normalize(z_j, dim=1)\n","\n","        # Loss calculation by nt_xent_loss function\n","        loss = nt_xent_loss(z_i, z_j, self.temperature)\n","        return loss\n","# Create the SimCLR model\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"RSZLTV3vRSv8"},"outputs":[],"source":["# define the Lars optimizer from scratch\n","# As the the coursework limit to use of 4 external libraries, we will implement the LARS optimizer from scratch\n","\n","from torch.optim.optimizer import Optimizer, required\n","import re\n","\n","EETA_DEFAULT = 0.001\n","\n","\n","class LARS(Optimizer):\n","    \"\"\"\n","    Layer-wise Adaptive Rate Scaling for large batch training.\n","    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n","    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        params,\n","        lr=required,\n","        momentum=0.9,\n","        use_nesterov=False,\n","        weight_decay=1e-6,\n","        exclude_from_weight_decay=None,\n","        exclude_from_layer_adaptation=None,\n","        classic_momentum=True,\n","        eeta=EETA_DEFAULT,\n","    ):\n","        \"\"\"Constructs a LARSOptimizer.\n","        Args:\n","        lr: A `float` for learning rate.\n","        momentum: A `float` for momentum.\n","        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n","        weight_decay: A `float` for weight decay.\n","        exclude_from_weight_decay: A list of `string` for variable screening, if\n","            any of the string appears in a variable's name, the variable will be\n","            excluded for computing weight decay. For example, one could specify\n","            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n","            from weight decay.\n","        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n","            for layer adaptation. If it is None, it will be defaulted the same as\n","            exclude_from_weight_decay.\n","        classic_momentum: A `boolean` for whether to use classic (or popular)\n","            momentum. The learning rate is applied during momeuntum update in\n","            classic momentum, but after momentum for popular momentum.\n","        eeta: A `float` for scaling of learning rate when computing trust ratio.\n","        name: The name for the scope.\n","        \"\"\"\n","\n","        self.epoch = 0\n","        defaults = dict(\n","            lr=lr,\n","            momentum=momentum,\n","            use_nesterov=use_nesterov,\n","            weight_decay=weight_decay,\n","            exclude_from_weight_decay=exclude_from_weight_decay,\n","            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n","            classic_momentum=classic_momentum,\n","            eeta=eeta,\n","        )\n","\n","        super(LARS, self).__init__(params, defaults)\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.weight_decay = weight_decay\n","        self.use_nesterov = use_nesterov\n","        self.classic_momentum = classic_momentum\n","        self.eeta = eeta\n","        self.exclude_from_weight_decay = exclude_from_weight_decay\n","        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n","        # arg is None.\n","        if exclude_from_layer_adaptation:\n","            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n","        else:\n","            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n","\n","    def step(self, epoch=None, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        if epoch is None:\n","            epoch = self.epoch\n","            self.epoch += 1\n","\n","        for group in self.param_groups:\n","            weight_decay = group[\"weight_decay\"]\n","            momentum = group[\"momentum\"]\n","            eeta = group[\"eeta\"]\n","            lr = group[\"lr\"]\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None:\n","                    continue\n","\n","                param = p.data\n","                grad = p.grad.data\n","\n","                param_state = self.state[p]\n","\n","                # TODO: get param names\n","                # if self._use_weight_decay(param_name):\n","                grad += self.weight_decay * param\n","\n","                if self.classic_momentum:\n","                    trust_ratio = 1.0\n","\n","                    # TODO: get param names\n","                    # if self._do_layer_adaptation(param_name):\n","                    w_norm = torch.norm(param)\n","                    g_norm = torch.norm(grad)\n","\n","                    # device = g_norm.get_device()\n","                    device = g_norm.device\n","                    trust_ratio = torch.where(\n","                        w_norm.gt(0),\n","                        torch.where(\n","                            g_norm.gt(0),\n","                            (self.eeta * w_norm / g_norm),\n","                            torch.Tensor([1.0]).to(device),\n","                        ),\n","                        torch.Tensor([1.0]).to(device),\n","                    ).item()\n","\n","                    scaled_lr = lr * trust_ratio\n","                    if \"momentum_buffer\" not in param_state:\n","                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n","                            p.data\n","                        )\n","                    else:\n","                        next_v = param_state[\"momentum_buffer\"]\n","\n","                    next_v.mul_(momentum).add_(scaled_lr, grad)\n","                    if self.use_nesterov:\n","                        update = (self.momentum * next_v) + (scaled_lr * grad)\n","                    else:\n","                        update = next_v\n","\n","                    p.data.add_(-update)\n","                else:\n","                    raise NotImplementedError\n","\n","        return loss\n","\n","    def _use_weight_decay(self, param_name):\n","        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n","        if not self.weight_decay:\n","            return False\n","        if self.exclude_from_weight_decay:\n","            for r in self.exclude_from_weight_decay:\n","                if re.search(r, param_name) is not None:\n","                    return False\n","        return True\n","\n","    def _do_layer_adaptation(self, param_name):\n","        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n","        if self.exclude_from_layer_adaptation:\n","            for r in self.exclude_from_layer_adaptation:\n","                if re.search(r, param_name) is not None:\n","                    return False\n","        return True\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XfGKQ-uJRSv9"},"source":["# Start training the model"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1335121,"status":"ok","timestamp":1711736828077,"user":{"displayName":"Jingfang Gu","userId":"08830157379463741241"},"user_tz":0},"id":"WUnnMYCWRSv9","outputId":"4c281f96-20d3-422a-c4ae-318c47c8c578"},"outputs":[],"source":["# Create directory to save the model\n","directory_name = 'models_attempt_1'\n","os.makedirs(directory_name, exist_ok=True)\n","# Start the training loop for SimCLR\n","model = resnet50(pretrained=False)\n","model.fc = nn.Identity()\n","simclr_model = SimCLR(model)\n","# Hyperparameters\n","accumulation_steps = 8  # For example, accumulate gradients over 4 steps before updating model weights\n","batch_size = 120\n","# The authors of the paper suggest lr = 0.3 x batchsize/256\n","# learning_rate = 0.1 * (batch_size * accumulation_steps) / 256\n","learning_rate = 0.3\n","num_epochs = 20\n","\n","\n","# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# move the model to the device\n","model = model.to(device)  # this is the model you want to save for pre-training, where f(.) is the ResNet-50\n","simclr_model = simclr_model.to(device)\n","\n","# define the dataset with and without data augmentation and with\n","dataset = preprocess(load_dataset(\"cats_vs_dogs\")['train'], device = device)\n","dataset = SimClrData(huggingface_dataset=dataset)\n","train_loader = DataLoader(dataset, batch_size=batch_size, shuffle = True)\n","\n","\n","# Define the optimizer\n","# optimizer = LARS(\n","#     [params for params in model.parameters() if params.requires_grad],\n","#     lr=learning_rate,\n","#     weight_decay=1e-6,\n","#     exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n","# )\n","\n","optimizer = LARS(simclr_model.parameters(), lr=0.1, momentum=0.9)\n","\n","# Start the training loop for SimCLR\n","\n","best_loss = float('inf')\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()  # Initialize gradient accumulation\n","    for step, data in enumerate(train_loader):\n","        images_i = data[0].to(device)\n","        # images_i = images_i.to(device)\n","        # Assuming data_transforms is a function that applies data augmentation\n","        images_j = data[1].to(device)\n","        # print(images_i.shape)\n","        \n","        loss = simclr_model(images_i, images_j) / accumulation_steps  # Normalize the loss by accumulation steps\n","        loss.backward()  # Accumulate gradients\n","        \n","        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n","            optimizer.step()  # Update model weights\n","            optimizer.zero_grad()  # Reset gradients\n","            \n","            print(f'Step {step+1}, Loss: {loss.item()}')\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n","    # Save the model after each epoch name with the epoch number\n","    torch.save(model.state_dict(), f'{directory_name}/simclr_backbone_{epoch}.ckpt')\n","    torch.save(simclr_model.state_dict(), f'{directory_name}/simclr_model_{epoch}.ckpt')\n","    # save the best model\n","    if loss < best_loss:\n","        torch.save(model.state_dict(), f'{directory_name}/best_simclr_backbone.ckpt')\n","        torch.save(simclr_model.state_dict(), f'{directory_name}/best_simclr_model.ckpt')\n","        best_loss = loss\n","\n","\n","\n","# Save the model outside the loop\n","torch.save(model.state_dict(), 'simclr_backbone.ckpt')\n","torch.save(simclr_model.state_dict(), 'simclr_model.ckpt')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
